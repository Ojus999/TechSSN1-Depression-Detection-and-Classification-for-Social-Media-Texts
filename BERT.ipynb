{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqNcsAVD_GNs"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the training dataset\n"
      ],
      "metadata": {
        "id": "XgcVgf1eASmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = pd.read_csv('train.csv')  # Replace 'train_dataset.csv' with the actual file name\n",
        "\n",
        "# Load the validation dataset\n",
        "val_dataset = pd.read_csv('dev_data.csv')  # Replace 'validation_dataset.csv' with the actual file name\n",
        "\n",
        "# Separate the text and label columns in the training dataset\n",
        "train_texts = train_dataset['Text_data'].tolist()\n",
        "train_labels = train_dataset['Label'].tolist()\n",
        "\n",
        "# Separate the text and label columns in the validation dataset\n",
        "val_texts = val_dataset['Text_data'].tolist()\n",
        "val_labels = val_dataset['Label'].tolist()\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the texts and convert them to input features\n",
        "train_input_ids = []\n",
        "train_attention_masks = []\n",
        "\n",
        "for text in train_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    train_input_ids.append(encoded_dict['input_ids'])\n",
        "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "val_input_ids = []\n",
        "val_attention_masks = []\n",
        "\n",
        "for text in val_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_input_ids.append(encoded_dict['input_ids'])\n",
        "    val_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the input features and labels to tensors\n",
        "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
        "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
        "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_attention_masks = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_attention_masks = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        _, predicted_labels = torch.max(probabilities, dim=1)\n",
        "\n",
        "        val_predictions.extend(predicted_labels.cpu().tolist())\n",
        "        val_true_labels.extend(batch_labels.cpu().tolist())\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(val_true_labels, val_predictions, target_names=label_encoder.classes_)\n",
        "    print('Classification Report:\\n', report)\n",
        "    print('-----------------------------')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'bert_model.pth')\n",
        "\n",
        "# Load the saved model for prediction on test data\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "model.load_state_dict(torch.load('bert_model.pth'))\n",
        "model.to(device)\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = pd.read_csv('test_data.csv')  # Replace 'test_data.csv' with the actual file name\n",
        "test_texts = test_dataset['Text_data'].fillna('').tolist()\n",
        "# Separate the text column in the test dataset\n",
        "#test_texts = test_dataset['Text_data'].tolist()\n",
        "\n",
        "# Tokenize the texts in the test dataset\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "\n",
        "for text in test_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the input features to tensors\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "\n",
        "# Create DataLoader for the test set\n",
        "test_dataloader = DataLoader(TensorDataset(test_input_ids, test_attention_masks), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Predict labels for the test dataset\n",
        "test_predictions = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch_input_ids = batch[0].to(device)\n",
        "    batch_attention_masks = batch[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    _, predicted_labels = torch.max(probabilities, dim=1)\n",
        "\n",
        "    test_predictions.extend(predicted_labels.cpu().tolist())\n",
        "\n",
        "# Decode the predicted labels using the label encoder\n",
        "test_predicted_labels = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "# Add the predicted labels as a column in the test dataset\n",
        "test_dataset['predicted_label'] = test_predicted_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "7dfnSknYBTCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "rwLpNlwUldtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Save the test dataset with predicted labels\n",
        "test_dataset.to_csv('test_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "HWs_OZyVit-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "qn2nGJW5ZWqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Read the predicted CSV file\n",
        "predicted_df = pd.read_csv('test_predictions.csv')\n",
        "\n",
        "# Read the actual labels CSV file\n",
        "actual_df = pd.read_csv('actual_test_data.csv')\n",
        "\n",
        "# Merge the two dataframes on the text column\n",
        "merged_df = pd.merge(predicted_df, actual_df, on='Text_data')\n",
        "\n",
        "# Extract the predicted and actual labels\n",
        "predicted_labels = merged_df['predicted_label']\n",
        "actual_labels = merged_df['Label']\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(actual_labels, predicted_labels)\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "SpILWFAQgeKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}